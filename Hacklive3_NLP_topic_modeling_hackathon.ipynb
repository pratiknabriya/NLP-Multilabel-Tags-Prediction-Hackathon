{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Hacklive3_NLP_topic_modeling",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMoQ-nPecHyd"
      },
      "source": [
        "# HACKLIVE 3: NLP HACKATHON \n",
        "## Topic Modeling for Research Articles 2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSznyeUQ_8Bv",
        "outputId": "0ae67f0c-bd0a-4df4-e736-ff0427a4548b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "# importing libraries \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#-----------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#-----------------------------------\n",
        "%matplotlib inline\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected = True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "import os\n",
        "import gc\n",
        "#-----------------------------------\n",
        "import re\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "#------------------------------------\n",
        "#from sklearn.manifold import TSNE\n",
        "#Import the Required lib packages for Word-Cloud generation\n",
        "# https://stackoverflow.com/questions/45625434/how-to-install-wordcloud-in-python3-6\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from os import path\n",
        "#----------------------------------- \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#-----------------------------------\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "#-----------------------------------\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDPz40WR_hw4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C21zcmFY_pvS"
      },
      "source": [
        "# loading data in Pandas dataframe from csv files\n",
        "\n",
        "my_path = '/content/gdrive/My Drive/JanataHack/NLP_topic_modeling_hackathon_23102020/'\n",
        "train_data = pd.read_csv(my_path + 'Train.csv')\n",
        "test_data = pd.read_csv(my_path + 'Test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQqzG7k5JWVW",
        "outputId": "3d018dd9-0467-4e78-8db2-c9100f69fb25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "# train data \n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Analysis of PDEs</th>\n",
              "      <th>Applications</th>\n",
              "      <th>Artificial Intelligence</th>\n",
              "      <th>Astrophysics of Galaxies</th>\n",
              "      <th>Computation and Language</th>\n",
              "      <th>Computer Vision and Pattern Recognition</th>\n",
              "      <th>Cosmology and Nongalactic Astrophysics</th>\n",
              "      <th>Data Structures and Algorithms</th>\n",
              "      <th>Differential Geometry</th>\n",
              "      <th>Earth and Planetary Astrophysics</th>\n",
              "      <th>Fluid Dynamics</th>\n",
              "      <th>Information Theory</th>\n",
              "      <th>Instrumentation and Methods for Astrophysics</th>\n",
              "      <th>Machine Learning</th>\n",
              "      <th>Materials Science</th>\n",
              "      <th>Methodology</th>\n",
              "      <th>Number Theory</th>\n",
              "      <th>Optimization and Control</th>\n",
              "      <th>Representation Theory</th>\n",
              "      <th>Robotics</th>\n",
              "      <th>Social and Information Networks</th>\n",
              "      <th>Statistics Theory</th>\n",
              "      <th>Strongly Correlated Electrons</th>\n",
              "      <th>Superconductivity</th>\n",
              "      <th>Systems and Control</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1824</td>\n",
              "      <td>a ever-growing datasets inside observational a...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3094</td>\n",
              "      <td>we propose the framework considering optimal $...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8463</td>\n",
              "      <td>nanostructures with open shell transition meta...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2082</td>\n",
              "      <td>stars are self-gravitating fluids inside which...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8687</td>\n",
              "      <td>deep neural perception and control networks ar...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  ... Systems and Control\n",
              "0  1824  ...                   0\n",
              "1  3094  ...                   0\n",
              "2  8463  ...                   0\n",
              "3  2082  ...                   0\n",
              "4  8687  ...                   0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN3kqODGLFIo",
        "outputId": "8e30b141-5ae6-4348-a3c9-4261fb5d76b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# test data\n",
        "\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Statistics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9409</td>\n",
              "      <td>fundamental frequency (f0) approximation from ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17934</td>\n",
              "      <td>this large-scale study, consisting of 24.5 mil...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16071</td>\n",
              "      <td>we present a stability analysis of the plane c...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16870</td>\n",
              "      <td>we construct finite time blow-up solutions to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10496</td>\n",
              "      <td>planetary nebulae (pne) constitute an importan...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ... Statistics\n",
              "0   9409  ...          1\n",
              "1  17934  ...          1\n",
              "2  16071  ...          0\n",
              "3  16870  ...          0\n",
              "4  10496  ...          0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5ZzzywzKjfn",
        "outputId": "8930ccb8-5782-4713-920d-d990a39ff6cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# train size and test size \n",
        "\n",
        "print(\"Train size:\", train_data.shape)\n",
        "print(\"Test size:\", test_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: (14004, 31)\n",
            "Test size: (6002, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ruWfbD9Lj5A",
        "outputId": "6ad830a8-1440-45a3-fc5d-c6aff8d09d38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train data info \n",
        "train_data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14004 entries, 0 to 14003\n",
            "Data columns (total 31 columns):\n",
            " #   Column                                        Non-Null Count  Dtype \n",
            "---  ------                                        --------------  ----- \n",
            " 0   id                                            14004 non-null  int64 \n",
            " 1   ABSTRACT                                      14004 non-null  object\n",
            " 2   Computer Science                              14004 non-null  int64 \n",
            " 3   Mathematics                                   14004 non-null  int64 \n",
            " 4   Physics                                       14004 non-null  int64 \n",
            " 5   Statistics                                    14004 non-null  int64 \n",
            " 6   Analysis of PDEs                              14004 non-null  int64 \n",
            " 7   Applications                                  14004 non-null  int64 \n",
            " 8   Artificial Intelligence                       14004 non-null  int64 \n",
            " 9   Astrophysics of Galaxies                      14004 non-null  int64 \n",
            " 10  Computation and Language                      14004 non-null  int64 \n",
            " 11  Computer Vision and Pattern Recognition       14004 non-null  int64 \n",
            " 12  Cosmology and Nongalactic Astrophysics        14004 non-null  int64 \n",
            " 13  Data Structures and Algorithms                14004 non-null  int64 \n",
            " 14  Differential Geometry                         14004 non-null  int64 \n",
            " 15  Earth and Planetary Astrophysics              14004 non-null  int64 \n",
            " 16  Fluid Dynamics                                14004 non-null  int64 \n",
            " 17  Information Theory                            14004 non-null  int64 \n",
            " 18  Instrumentation and Methods for Astrophysics  14004 non-null  int64 \n",
            " 19  Machine Learning                              14004 non-null  int64 \n",
            " 20  Materials Science                             14004 non-null  int64 \n",
            " 21  Methodology                                   14004 non-null  int64 \n",
            " 22  Number Theory                                 14004 non-null  int64 \n",
            " 23  Optimization and Control                      14004 non-null  int64 \n",
            " 24  Representation Theory                         14004 non-null  int64 \n",
            " 25  Robotics                                      14004 non-null  int64 \n",
            " 26  Social and Information Networks               14004 non-null  int64 \n",
            " 27  Statistics Theory                             14004 non-null  int64 \n",
            " 28  Strongly Correlated Electrons                 14004 non-null  int64 \n",
            " 29  Superconductivity                             14004 non-null  int64 \n",
            " 30  Systems and Control                           14004 non-null  int64 \n",
            "dtypes: int64(30), object(1)\n",
            "memory usage: 3.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SQeeedtMVQQ",
        "outputId": "0e2e8e4b-5fee-4949-c18e-ebf933dd5cf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# test data info \n",
        "test_data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6002 entries, 0 to 6001\n",
            "Data columns (total 6 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   id                6002 non-null   int64 \n",
            " 1   ABSTRACT          6002 non-null   object\n",
            " 2   Computer Science  6002 non-null   int64 \n",
            " 3   Mathematics       6002 non-null   int64 \n",
            " 4   Physics           6002 non-null   int64 \n",
            " 5   Statistics        6002 non-null   int64 \n",
            "dtypes: int64(5), object(1)\n",
            "memory usage: 281.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jC_6nYhSsBG",
        "outputId": "bcee8c33-c341-4ebe-add2-9383b7ef858a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# check for null values of any \n",
        "\n",
        "print(train_data.isnull().sum())\n",
        "print(\"- \"* 50)\n",
        "print(test_data.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id                                              0\n",
            "ABSTRACT                                        0\n",
            "Computer Science                                0\n",
            "Mathematics                                     0\n",
            "Physics                                         0\n",
            "Statistics                                      0\n",
            "Analysis of PDEs                                0\n",
            "Applications                                    0\n",
            "Artificial Intelligence                         0\n",
            "Astrophysics of Galaxies                        0\n",
            "Computation and Language                        0\n",
            "Computer Vision and Pattern Recognition         0\n",
            "Cosmology and Nongalactic Astrophysics          0\n",
            "Data Structures and Algorithms                  0\n",
            "Differential Geometry                           0\n",
            "Earth and Planetary Astrophysics                0\n",
            "Fluid Dynamics                                  0\n",
            "Information Theory                              0\n",
            "Instrumentation and Methods for Astrophysics    0\n",
            "Machine Learning                                0\n",
            "Materials Science                               0\n",
            "Methodology                                     0\n",
            "Number Theory                                   0\n",
            "Optimization and Control                        0\n",
            "Representation Theory                           0\n",
            "Robotics                                        0\n",
            "Social and Information Networks                 0\n",
            "Statistics Theory                               0\n",
            "Strongly Correlated Electrons                   0\n",
            "Superconductivity                               0\n",
            "Systems and Control                             0\n",
            "dtype: int64\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "id                  0\n",
            "ABSTRACT            0\n",
            "Computer Science    0\n",
            "Mathematics         0\n",
            "Physics             0\n",
            "Statistics          0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2in-VkVoJyO4",
        "outputId": "8c68ff0b-cf5c-4bde-e344-dac20a639bcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# how many test points have multiple tags \n",
        "\n",
        "count = 0\n",
        "for i in range(train_data.shape[0]):\n",
        "    if sum(train_data.iloc[i, 6:]) > 1:\n",
        "        count += 1\n",
        "\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xNJiB34N7J-"
      },
      "source": [
        "Out of total train points, 4247 have more than 1 tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5azJaQVpLqQ",
        "outputId": "7440a075-f061-47f2-b234-3bc5c1328610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# number of samples with more than one tag\n",
        "my_list = []\n",
        "for i in range(train_data.shape[0]):\n",
        "    my_list.append(sum(train_data.iloc[i, 6:]))\n",
        "\n",
        "pd.Series(my_list).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    9757\n",
              "2    3744\n",
              "3     465\n",
              "4      38\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3vg31LNPi-L",
        "outputId": "315c230f-1e8f-4415-e251-4ded74bf17d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "sns.set(style = 'whitegrid')\n",
        "\n",
        "my_dict = dict(pd.Series(my_list).value_counts())\n",
        "plt.figure(figsize = (8, 5))\n",
        "plt.bar(my_dict.keys(), my_dict.values(), width = 0.4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAExCAYAAAB/Dds9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVm0lEQVR4nO3dX2zV9f3H8dc5p38YaD0e1pZDIbKRYc7WZY42IdnmhbDZZimFG9OmcRcCEsf+dIt/Wqe2HbDFFkZk/JnojBdmU8PFMC2OoumSCcsMnojZoUYIax2jhzae1ogdnuLp93dhPL91LXB6/vScvs/zcUXP55yT95vmy5NzWorLcRxHAADAHHe2BwAAAJlB5AEAMIrIAwBgFJEHAMAoIg8AgFEF2R4gnSYnJzU+Pq7CwkK5XK5sjwMAQEY5jqOrV69q0aJFcrunv243Ffnx8XGdPXs222MAADCnVq1apZtvvnna7aYiX1hYKOmzZYuKirI8TWJCoZAqKyuzPcacyad982lXiX0ty6ddpfm178TEhM6ePRvv3/8yFfnP36IvKipScXFxlqdJ3HyaNR3yad982lViX8vyaVdp/u17rS9R3/Ab7zo7O7V27VrdfvvtU94KHxgYUENDg2pqatTQ0KDBwcGMngEAgNm5YeTXrVunP/zhD6qoqJhye3t7u5qamtTb26umpia1tbVl9AwAAMzODSNfXV0tv98/5bZIJKL+/n7V1dVJkurq6tTf36/R0dGMnAEAgNlL6mvy4XBY5eXl8ng8kiSPx6OysjKFw2E5jpP2M5/PN6v5QqFQMmtlTTAYzPYIcyqf9s2nXSX2tSyfdpXs7GvqG+8+V1lZOW++aSIYDKqqqirbY8yZfNo3n3aV2NeyfNpVml/7RqPR676wTSryfr9fw8PDisVi8ng8isViGhkZkd/vl+M4aT8DAACzl9SPtV28eLECgYB6enokST09PQoEAvL5fBk5AwAAs3fDV/I7d+7U8ePH9cEHH+i+++6T1+vV0aNH1dHRodbWVh08eFAlJSXq7OyMPyYTZwAAYHZuGPnHH39cjz/++LTbV65cqcOHD8/4mEycAQCA2eF/obuBiauxjD5/pr+5I9PzAwByl8nvrk+nokKP1j/4SrbHSFr3bzZkewQAQJbwSh4AAKOIPAAARhF5AACMIvIAABhF5AEAMIrIAwBgFJEHAMAoIg8AgFFEHgAAo4g8AABGEXkAAIwi8gAAGEXkAQAwisgDAGAUkQcAwCgiDwCAUUQeAACjiDwAAEYReQAAjCLyAAAYReQBADCKyAMAYBSRBwDAKCIPAIBRRB4AAKOIPAAARhF5AACMIvIAABhF5AEAMIrIAwBgFJEHAMAoIg8AgFFEHgAAo4g8AABGEXkAAIwi8gAAGEXkAQAwisgDAGAUkQcAwCgiDwCAUUQeAACjiDwAAEalHPm//OUv2rhxozZs2KD6+nodP35ckjQwMKCGhgbV1NSooaFBg4OD8cckewYAABKXUuQdx9Ejjzyirq4uvfLKK+rq6lJLS4smJyfV3t6upqYm9fb2qqmpSW1tbfHHJXsGAAASl/IrebfbrcuXL0uSLl++rLKyMo2Njam/v191dXWSpLq6OvX392t0dFSRSCSpMwAAMDsFqTzY5XLpqaee0rZt27Rw4UKNj4/rmWeeUTgcVnl5uTwejyTJ4/GorKxM4XBYjuMkdebz+RKeKxQKpbLWFFVVVWl7rmwJBoPZHmGKXJsnk/JpV4l9LcunXSU7+6YU+U8//VSHDh3SwYMHVVVVpWAwqJ/97Gfq6upK13xJqaysVHFxcVZnyCW59BeVYDCYU/NkUj7tKrGvZfm0qzS/9o1Go9d9YZtS5N99912NjIzEfzOqqqr0hS98QcXFxRoeHlYsFpPH41EsFtPIyIj8fr8cx0nqDAAAzE5KX5NfsmSJLl26pH/+85+SpPPnzysSiei2225TIBBQT0+PJKmnp0eBQEA+n0+LFy9O6gwAAMxOSq/kS0tL1dHRoebmZrlcLknSr3/9a3m9XnV0dKi1tVUHDx5USUmJOjs7449L9gwAACQupchLUn19verr66fdvnLlSh0+fHjGxyR7BgAAEsdPvAMAwCgiDwCAUUQeAACjiDwAAEYReQAAjCLyAAAYReQBADCKyAMAYBSRBwDAKCIPAIBRRB4AAKOIPAAARhF5AACMIvIAABhF5AEAMIrIAwBgFJEHAMAoIg8AgFFEHgAAo4g8AABGEXkAAIwi8gAAGEXkAQAwisgDAGAUkQcAwCgiDwCAUUQeAACjiDwAAEYReQAAjCLyAAAYReQBADCKyAMAYBSRBwDAKCIPAIBRRB4AAKOIPAAARhF5AACMIvIAABhF5AEAMIrIAwBgFJEHAMAoIg8AgFFEHgAAo4g8AABGpRz5aDSq9vZ23X333Vq/fr2eeOIJSdLAwIAaGhpUU1OjhoYGDQ4Oxh+T7BkAAEhcypHftWuXiouL1dvbq+7ubjU3N0uS2tvb1dTUpN7eXjU1NamtrS3+mGTPAABA4lKK/Pj4uI4cOaLm5ma5XC5J0he/+EVFIhH19/errq5OklRXV6f+/n6Njo4mfQYAAGanIJUHX7hwQV6vV/v379ebb76pRYsWqbm5WQsWLFB5ebk8Ho8kyePxqKysTOFwWI7jJHXm8/kSnisUCqWy1hRVVVVpe65sCQaD2R5hilybJ5PyaVeJfS3Lp10lO/umFPlYLKYLFy7oq1/9qlpaWvTOO+/ogQce0N69e9M1X1IqKytVXFyc1RlySS79RSUYDObUPJmUT7tK7GtZPu0qza99o9HodV/YphR5v9+vgoKC+Nvr3/jGN3TrrbdqwYIFGh4eViwWk8fjUSwW08jIiPx+vxzHSeoMAADMTkpfk/f5fFqzZo1Onjwp6bPvjI9EIlqxYoUCgYB6enokST09PQoEAvL5fFq8eHFSZwAAYHZSeiUvSb/85S/1i1/8Qp2dnSooKFBXV5dKSkrU0dGh1tZWHTx4UCUlJers7Iw/JtkzAACQuJQjv3z5cr3wwgvTbl+5cqUOHz4842OSPQMAAInjJ94BAGAUkQcAwCgiDwCAUUQeAACjiDwAAEYReQAAjCLyAAAYReQBADCKyAMAYBSRBwDAKCIPAIBRRB4AAKOIPAAARhF5AACMIvIAABhF5AEAMIrIAwBgFJEHAMAoIg8AgFFEHgAAo4g8AABGEXkAAIwi8gAAGEXkAQAwisgDAGAUkQcAwCgiDwCAUUQeAACjiDwAAEYReQAAjCLyAAAYReQBADCKyAMAYBSRBwDAKCIPAIBRRB4AAKOIPAAARhF5AACMIvIAABhF5AEAMIrIAwBgFJEHAMAoIg8AgFFEHgAAo9IW+f379+v222/X2bNnJUmnT59WfX29ampqtGnTJkUikfh9kz0DAACJS0vkz5w5o9OnT6uiokKSNDk5qYcfflhtbW3q7e1VdXW1du/endIZAACYnZQjPzExoe3bt6ujoyN+WygUUnFxsaqrqyVJjY2NOnbsWEpnAABgdgpSfYK9e/eqvr5ey5Yti98WDoe1dOnS+Mc+n0+Tk5P68MMPkz7zer0JzxQKhVLc6v9VVVWl7bmyJRgMZnuEKXJtnkzKp10l9rUsn3aV7OybUuTffvtthUIhPfTQQ+maJy0qKytVXFyc7TFyRi79RSUYDObUPJmUT7tK7GtZPu0qza99o9HodV/YphT5U6dO6fz581q3bp0k6dKlS9q8ebN+8IMfaGhoKH6/0dFRud1ueb1e+f3+pM4AAMDspPQ1+a1bt+rEiRPq6+tTX1+flixZoueee05btmzRJ598orfeekuS9NJLL6m2tlbSZ6+ykzkDAACzk/LX5GfidrvV1dWl9vZ2RaNRVVRUaNeuXSmdAQCA2Ulr5Pv6+uK/Xr16tbq7u2e8X7JnAAAgcfzEOwAAjCLyyFsTV2MZff5Mf3dupucHMP9l5GvywHxQVOjR+gdfyfYYSev+zYZsjwAgx/FKHgAAo4g8AABGEXkAAIwi8gAAGEXkAQAwisgDAGAUkQcAwCgiDwCAUUQeAACjiDwAAEYReQAAjCLyAAAYReQBADCKyAMAYBSRBwDAKCIPAIBRRB4AAKOIPAAARhF5AACMIvIAABhF5AEAMIrIAwBgFJEHAMAoIg8AgFFEHgAAo4g8AABGEXkAAIwi8gAAGEXkAQAwisgDAGAUkQcAwCgiDwCAUUQeAACjiDwAAEYReQAAjCLyAAAYReQBADCKyAMAYBSRBwDAKCIPAIBRRB4AAKNSivzY2Jjuv/9+1dTUaP369frxj3+s0dFRSdLp06dVX1+vmpoabdq0SZFIJP64ZM8AAEDiUoq8y+XSli1b1Nvbq+7ubi1fvly7d+/W5OSkHn74YbW1tam3t1fV1dXavXu3JCV9BgAAZielyHu9Xq1Zsyb+8R133KGhoSGFQiEVFxerurpaktTY2Khjx45JUtJnAABgdgrS9USTk5N68cUXtXbtWoXDYS1dujR+5vP5NDk5qQ8//DDpM6/Xm/AsoVAoPUtJqqqqSttzZUswGMz2CFPkyjx8btMv1+bJtHzaN592lezsm7bI79ixQwsXLtS9996r1157LV1Pm5TKykoVFxdndYZckksxCwaDOTXPfJdLv5f59rnNp33zaVdpfu0bjUav+8I2LZHv7OzU+++/r6efflput1t+v19DQ0Px89HRUbndbnm93qTPAADA7KT8T+j27NmjUCikAwcOqKioSNJnr6Q/+eQTvfXWW5Kkl156SbW1tSmdAQCA2Unplfy5c+d06NAhrVixQo2NjZKkZcuW6cCBA+rq6lJ7e7ui0agqKiq0a9cuSZLb7U7qDAAAzE5Kkf/KV76i9957b8az1atXq7u7O61nAAAgcfzEOwAAjCLyAAAYReQBADCKyAMAYBSRBwDAKCIPAIBRRB4AAKOIPAAARhF5AACMIvIAABhF5AEAMIrIAwBgFJEHAMAoIg8AgFFEHgAAo4g8AABGEXkAAIwi8gAAGEXkAQAwisgDAGAUkQcAwCgiDwCAUUQeAACjiDwAAEYReQAAjCLyAAAYReQBADCKyAMAYBSRBwDAKCIPAIBRRB4AAKOIPAAARhF5AACMIvIAABhF5AEAMIrIAzBp4moso89fVVWVsefO9OzIHwXZHgAAMqGo0KP1D76S7TGS0v2bDdkeAUbwSh4AAKOIPAAARhF5AACMIvIAABhF5AEAMIrIAwBgFJEHAMConIz8wMCAGhoaVFNTo4aGBg0ODmZ7JAAA5p2cjHx7e7uamprU29urpqYmtbW1ZXskAADmnZz7iXeRSET9/f16/vnnJUl1dXXasWOHRkdH5fP5rvtYx3EkSRMTE2mdybvIk9bnm0vRaDTbI0yTSzPxuU2vXJtpvn5+c+33Ucqtma5+GlNhQeY+t5WVlRndN53zf967z/v3v1zOtU6yJBQKqaWlRUePHo3f9v3vf1+7du3S1772tes+9vLlyzp79mymRwQAIKesWrVKN99887Tbc+6VfCoWLVqkVatWqbCwUC6XK9vjAACQUY7j6OrVq1q0aNGM5zkXeb/fr+HhYcViMXk8HsViMY2MjMjv99/wsW63e8a/yQAAYNWCBQuueZZz33i3ePFiBQIB9fT0SJJ6enoUCARu+PV4AAAwVc59TV6Szp8/r9bWVn300UcqKSlRZ2envvzlL2d7LAAA5pWcjDwAAEhdzr1dDwAA0oPIAwBgFJEHAMAoIg8AgFFEHgAAo3Luh+FY09nZqd7eXl28eFHd3d1atWrVtPvEYjHt3LlTb7zxhlwul7Zu3ap77rknC9OmLpF99+3bpz/+8Y8qKyuTJK1evVrt7e1zPWrKxsbG9Mgjj+hf//qXioqKdNttt2n79u3TfqbDlStX9Oijj+rMmTPyeDxqaWnRXXfdlaWpk5fovq2trfrb3/6mW2+9VZJUW1urH/7wh9kYOWXbtm3Tv//9b7ndbi1cuFBPPPGEAoHAlPtYuX4T2dXKtfvf9u/fr3379s3455WJa9dBRp06dcoZGhpy7rrrLue9996b8T5/+tOfnE2bNjmxWMyJRCLOnXfe6Vy4cGGOJ02PRPb97W9/6zz55JNzPFn6jY2NOX//+9/jHz/55JPOo48+Ou1++/btcx577DHHcRxnYGDA+da3vuV8/PHHczZnuiS6b0tLi/PCCy/M5WgZ89FHH8V//dprrzkbN26cdh8r128iu1q5dj8XCoWczZs3X/PPKwvXLm/XZ1h1dfUNfyTvq6++qnvuuUdut1s+n0/f/e53dezYsTmaML0S2dcKr9erNWvWxD++4447NDQ0NO1+f/7zn9XQ0CBJWrFihSorK/XXv/51zuZMl0T3teS/f0z2xx9/POP/iWHl+k1kV0smJia0fft2dXR0XPM+Fq5d3q7PAeFwWEuXLo1/7Pf7denSpSxOlHlHjx7ViRMnVFpaqp/85Cf65je/me2RUjI5OakXX3xRa9eunXY2NDSkioqK+McWPr/X21eSnn/+eb388stavny5HnzwQa1cuXKOJ0yfxx57TCdPnpTjOPr9738/7dzS9XujXSU71+7evXtVX1+vZcuWXfM+Fq5dIo8519jYqAceeECFhYU6efKktm3bpldffTX+Ndz5aMeOHVq4cKHuvffebI8yJ663789//nOVlpbK7XbryJEj2rJli15//XV5PPPz/3b/1a9+JUk6cuSIurq69Oyzz2Z5osy50a5Wrt23335boVBIDz30ULZHyTjers8Bfr9/ytue4XBYS5YsyeJEmVVaWqrCwkJJ0re//W35/X6dO3cuy1Mlr7OzU++//76eeuopud3TL6mlS5fq4sWL8Y/n++f3RvuWl5fHb9+4caP+85//zLtXPzPZuHGj3nzzTY2NjU253eL1e61drVy7p06d0vnz57Vu3TqtXbtWly5d0ubNm3XixIkp97Nw7RL5HFBbW6vDhw9rcnJSo6Ojev3111VTU5PtsTJmeHg4/ut3331XFy9e1Je+9KUsTpS8PXv2KBQK6cCBAyoqKprxPrW1tXr55ZclSYODg/rHP/6hO++8cy7HTJtE9v3vz+8bb7wht9ut8vLyuRoxbcbHxxUOh+Mf9/X16ZZbbpHX651yPwvXb6K7Wrl2t27dqhMnTqivr099fX1asmSJnnvuOX3nO9+Zcj8L1y5v12fYzp07dfz4cX3wwQe677775PV6dfToUd1///366U9/qq9//evasGGD3nnnHd19992SpB/96Edavnx5lidPTiL77tmzR2fOnJHb7VZhYaG6urpUWlqa7dFn7dy5czp06JBWrFihxsZGSdKyZct04MABbdiwQc8884zKy8u1efNmtba26nvf+57cbre2b9+um266KcvTz16i+7a0tCgSicjlcummm27S7373OxUUzL8/aq5cuaLm5mZduXJFbrdbt9xyi55++mm5XC5z12+iu1q5dq/H2rXL/0IHAIBRvF0PAIBRRB4AAKOIPAAARhF5AACMIvIAABhF5AEAMIrIAwBg1P8B6hhk2r3x3MwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "picYE5qx2bly"
      },
      "source": [
        "id_col = 'id'\n",
        "\n",
        "topic_cols = ['Computer Science', 'Mathematics', 'Physics', 'Statistics']\n",
        "\n",
        "target_cols = ['Analysis of PDEs', 'Applications',\n",
        "               'Artificial Intelligence', 'Astrophysics of Galaxies',\n",
        "               'Computation and Language', 'Computer Vision and Pattern Recognition',\n",
        "               'Cosmology and Nongalactic Astrophysics',\n",
        "               'Data Structures and Algorithms', 'Differential Geometry',\n",
        "               'Earth and Planetary Astrophysics', 'Fluid Dynamics',\n",
        "               'Information Theory', 'Instrumentation and Methods for Astrophysics',\n",
        "               'Machine Learning', 'Materials Science', 'Methodology', 'Number Theory',\n",
        "               'Optimization and Control', 'Representation Theory', 'Robotics',\n",
        "               'Social and Information Networks', 'Statistics Theory',\n",
        "               'Strongly Correlated Electrons', 'Superconductivity',\n",
        "               'Systems and Control']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxjghX3tOmv_",
        "outputId": "d44bbebd-7738-4e94-f55e-8ea4e5ecbc3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "# observe few random abstract texts for deciding the preprocessing steps\n",
        "\n",
        "for i in range(1000, 14000, 2000):\n",
        "    print(train_data['ABSTRACT'][i])\n",
        "    print(\"- \"*50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "graph-based methods are known to be successful inside many machine learning and pattern classification tasks. these methods consider semi-structured data as graphs where nodes correspond to primitives (parts, interest points, segments, etc.) and edges characterize a relationships between these primitives. however, these non-vectorial graph data cannot be straightforwardly plugged into off-the-shelf machine learning algorithms without the preliminary step of -- explicit/implicit -- graph vectorization and embedding. this embedding process should be resilient to intra-class graph variations while being highly discriminant. inside this paper, we propose the novel high-order stochastic graphlet embedding (sge) that maps graphs into vector spaces. our main contribution includes the new stochastic search procedure that efficiently parses the given graph and extracts/samples unlimitedly high-order graphlets. we consider these graphlets, with increasing orders, to model local primitives as well as their increasingly complex interactions. inside order to build our graph representation, we measure a distribution of these graphlets into the given graph, with the help of particular hash functions that efficiently assign sampled graphlets into isomorphic sets with the very low probability of collision. when combined with maximum margin classifiers, these graphlet-based representations have positive impact on a performance of pattern comparison and recognition as corroborated through extensive experiments with the help of standard benchmark databases.\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "let $t$ be the circle and $lt$ be its loop group. let $\\mathcal{m}$ be an infinite dimensional manifold equipped with the nice $lt$-action. we construct an analytic $lt$-equivariant index considering $\\mathcal{m}$, and justify it inside terms of noncommutative geometry. more precisely, we construct the hilbert space $\\mathcal{h}$ consisting of \"$l^2$-sections of the clifford module bundle\" and the \"dirac operator\" $\\mathcal{d}$ which acts on $\\mathcal{h}$. then, we define an analytic index of $\\mathcal{d}$ valued inside a representation group of $lt$, so called verlinde ring. we also define the \"twisted crossed product $lt\\ltimes_\\tau c_0(\\mathcal{m})$,\" although we cannot define each concept \"function algebra considering $\\mathcal{m}$ vanishing at infinity,\" \"function from $lt$ to the $c^*$-algebra vanishing at infinity,\" and the haar measure on $lt$. moreover we combine all of them inside terms of spectral triples and verify that a triple has an infinite spectral dimension. lastly, we add some applications including borel-weil theory considering $lt$.\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "we study a efficient learnability of geometric concept classes - specifically, low-degree polynomial threshold functions (ptfs) and intersections of halfspaces - when the fraction of a data was adversarially corrupted. we give a first polynomial-time pac learning algorithms considering these concept classes with dimension-independent error guarantees inside a presence of nasty noise under a gaussian distribution. inside a nasty noise model, an omniscient adversary should arbitrarily corrupt the small fraction of both a unlabeled data points and their labels. this model generalizes well-studied noise models, including a malicious noise model and a agnostic (adversarial label noise) model. prior to our work, a only concept class considering which efficient malicious learning algorithms were known is a class of origin-centered halfspaces. specifically, our robust learning algorithm considering low-degree ptfs succeeds under the number of tame distributions -- including a gaussian distribution and, more generally, any log-concave distribution with (approximately) known low-degree moments. considering ltfs under a gaussian distribution, we give the polynomial-time algorithm that achieves error $o(\\epsilon)$, where $\\epsilon$ was a noise rate. at a core of our pac learning results was an efficient algorithm to approximate a low-degree chow-parameters of any bounded function inside a presence of nasty noise. to achieve this, we employ an iterative spectral method considering outlier detection and removal, inspired by recent work inside robust unsupervised learning. our aforementioned algorithm succeeds considering the range of distributions satisfying mild concentration bounds and moment assumptions. a correctness of our robust learning algorithm considering intersections of halfspaces makes essential use of the novel robust inverse independence lemma that may be of broader interest.\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "the continuously rotating half-wave plate (crhwp) was the promising tool to improve a sensitivity to large angular scales inside cosmic microwave background (cmb) polarization measurements. with the crhwp, single detectors should measure three of a stokes parameters, $i$, $q$ and $u$, thereby avoiding a set of systematic errors that should be introduced by mismatches inside a properties of orthogonal detector pairs. we focus on a implementation of crhwps inside large aperture telescopes (i.e. a primary mirror was larger than a current maximum half-wave plate diameter of $\\sim$0.5 m), where a crhwp should be placed between a primary mirror and focal plane. inside this configuration, one needs to address a intensity to polarization ($i{\\rightarrow}p$) leakage of a optics, which becomes the source of 1/f noise and also causes differential gain systematics that arise from cmb temperature fluctuations. inside this paper, we present a performance of the crhwp installed inside a polarbear experiment, which employs the gregorian telescope with the 2.5 m primary illumination pattern. a crhwp was placed near a prime focus between a primary and secondary mirrors. we find that a $i{\\rightarrow}p$ leakage was larger than a expectation from a physical properties of our primary mirror, resulting inside the 1/f knee of 100 mhz. a excess leakage could be due to imperfections inside a detector system, i.e. detector non-linearity inside a responsivity and time-constant. we demonstrate, however, that by subtracting a leakage correlated with a intensity signal, a 1/f noise knee frequency was reduced to 32 mhz ($\\ell \\sim$39 considering our scan strategy), which was very promising to probe a primordial b-mode signal. we also discuss methods considering further noise subtraction inside future projects where a precise temperature control of instrumental components and a leakage reduction will play the key role.\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "we address a problem of bootstrapping language acquisition considering an artificial system similarly to what was observed inside experiments with human infants. our method works by associating meanings to words inside manipulation tasks, as the robot interacts with objects and listens to verbal descriptions of a interactions. a model was based on an affordance network, i.e., the mapping between robot actions, robot perceptions, and a perceived effects of these actions upon objects. we extend a affordance model to incorporate spoken words, which allows us to ground a verbal symbols to a execution of actions and a perception of a environment. a model takes verbal descriptions of the task as a input and uses temporal co-occurrence to create links between speech utterances and a involved objects, actions, and effects. we show that a robot was able form useful word-to-meaning associations, even without considering grammatical structure inside a learning process and inside a presence of recognition errors. these word-to-meaning associations are embedded inside a robot's own understanding of its actions. thus, they should be directly used to instruct a robot to perform tasks and also allow to incorporate context inside a speech recognition task. we believe that a encouraging results with our idea behind the method may afford robots with the capacity to acquire language descriptors inside their operation's environment as well as to shed some light as to how this challenging process develops with human infants.\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "computationally efficient moving object detection and depth approximation from the stereo camera was an extremely useful tool considering many computer vision applications, including robotics and autonomous driving. inside this paper we show how moving objects should be densely detected by estimating disparity with the help of an algorithm that improves complexity and accuracy of stereo matching by relying on information from previous frames. a main idea behind this idea behind the method was that by with the help of a ego-motion approximation and a disparity map of a previous frame, we should set the prior base that enables us to reduce a complexity of a current frame disparity estimation, subsequently also detecting moving objects inside a scene. considering each pixel we run the kalman filter that recursively fuses a disparity prediction and reduced space semi-global matching (sgm) measurements. a proposed algorithm has been implemented and optimized with the help of streaming single instruction multiple data instruction set and multi-threading. furthermore, inside order to approximate a process and measurement noise as reliably as possible, we conduct extensive experiments on a kitti suite with the help of a ground truth obtained by a 3d laser range sensor. concerning disparity estimation, compared to a opencv sgm implementation, a proposed method yields improvement on a kitti dataset sequences inside terms of both speed and accuracy.\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "mass was the fundamental property of galaxy groups and clusters. inside theory weak gravitational lensing will enable an approximately unbiased measurement of mass, but parametric methods considering extracting cluster masses from data require a additional knowledge of concentration. measurements of both mass and concentration are limited by a degeneracy between a two parameters, particularly inside low mass, high redshift systems where a signal-to-noise was low. inside this paper we develop the hierarchical model of mass and concentration considering mass inference we test our method on toy data and then apply it to the sample of galaxy groups and poor clusters down to masses of $\\sim$ 1e13 m$_\\odot$. our fit and model gives the relationship among masses, concentrations and redshift that allow prediction of these parameters from incomplete and noisy future measurements. additionally a underlying population should be used to infer an observationally based concentration-mass relation. our method was equivalent to the quasi- stacking idea behind the method with a degree of stacking set by a data. we also demonstrate that mass and concentration derived from pure stacking should be offset from a population mean with differing values depending on a method of stacking.\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcQSt8ICU3Xv"
      },
      "source": [
        "# we need to decontract the english phrases within the questions \n",
        "# https://stackoverflow.com/a/47091490/4084039\n",
        "\n",
        "def decontracted(phrase):\n",
        "    #specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"cannot\", phrase)\n",
        "    \n",
        "    # general \n",
        "    phrase = re.sub(r\"n\\'t\", \" not\",   phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\",   phrase)\n",
        "    phrase = re.sub(r\"\\'s\",  \" is\",    phrase) \n",
        "    phrase = re.sub(r\"\\'d\",  \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\",  phrase)\n",
        "    phrase = re.sub(r\"\\'t\",  \" not\",   phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\",  phrase)\n",
        "    phrase = re.sub(r\"\\'m\",  \" am\",    phrase)\n",
        "    phrase = re.sub(r\"\\'em\", \" them\",  phrase)\n",
        "    return phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZC3T1QU53fx"
      },
      "source": [
        "'''from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "stopwords = set(list(STOPWORDS)) \n",
        "stopwords\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lBy61xKVlo5"
      },
      "source": [
        "# list of all the stopwords (except 'no', 'nor' and 'not') \n",
        "# https://gist.github.com/sebleier/554280\n",
        "\n",
        "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
        "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
        "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
        "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
        "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
        "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
        "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
        "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
        "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
        "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
        "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
        "            'won', \"won't\", 'wouldn', \"wouldn't\", 'would', 'could', 'br'] # 'br' added to handle line-breaks "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNFiVgAPlzM-",
        "outputId": "bf792825-b1bb-45a3-e1a7-ba9d9f055604",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Stemming using Porter Stemmer | reference: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def stemming(sentence):\n",
        "    token_words = word_tokenize(sentence)\n",
        "    stem_sentence = []\n",
        "    for word in token_words:  \n",
        "        stemmer = PorterStemmer()\n",
        "        stem_sentence.append(stemmer.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf5yafsAn976"
      },
      "source": [
        "# text preprocessing pipeline \n",
        "\n",
        "def text_preprocessing(text):\n",
        "    preprocessed_abstract = []\n",
        "    for sentence in tqdm(text):\n",
        "        sent = decontracted(sentence)\n",
        "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent) \n",
        "        sent = ' '.join(e.lower() for e in sent.split() if e.lower() not in stopwords)\n",
        "        sent = stemming(sent)\n",
        "        preprocessed_abstract.append(sent.strip())\n",
        "    return preprocessed_abstract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE9wdNutqNdH",
        "outputId": "2797897e-fcbf-4c11-b828-7b2d63ee13ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# preprocessing train data abstract text\n",
        "\n",
        "train_data['preprocessed_abstract'] = text_preprocessing(train_data['ABSTRACT'].values)\n",
        "\n",
        "train_data[['ABSTRACT', 'preprocessed_abstract']].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 14004/14004 [00:49<00:00, 281.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>preprocessed_abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a ever-growing datasets inside observational a...</td>\n",
              "      <td>ever grow dataset insid observ astronomi chall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>we propose the framework considering optimal $...</td>\n",
              "      <td>propos framework consid optim match exclud pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nanostructures with open shell transition meta...</td>\n",
              "      <td>nanostructur open shell transit metal molecula...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stars are self-gravitating fluids inside which...</td>\n",
              "      <td>star self gravit fluid insid pressur buoyanc r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>deep neural perception and control networks ar...</td>\n",
              "      <td>deep neural percept control network like key c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            ABSTRACT                              preprocessed_abstract\n",
              "0  a ever-growing datasets inside observational a...  ever grow dataset insid observ astronomi chall...\n",
              "1  we propose the framework considering optimal $...  propos framework consid optim match exclud pre...\n",
              "2  nanostructures with open shell transition meta...  nanostructur open shell transit metal molecula...\n",
              "3  stars are self-gravitating fluids inside which...  star self gravit fluid insid pressur buoyanc r...\n",
              "4  deep neural perception and control networks ar...  deep neural percept control network like key c..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTXqWCCZoKFC",
        "outputId": "de43e98d-e332-4675-9888-eb6215b4f2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# preprocessing test data abstract text\n",
        "\n",
        "test_data['preprocessed_abstract'] = text_preprocessing(test_data['ABSTRACT'].values)\n",
        "\n",
        "test_data[['ABSTRACT', 'preprocessed_abstract']].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 6002/6002 [00:21<00:00, 283.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>preprocessed_abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fundamental frequency (f0) approximation from ...</td>\n",
              "      <td>fundament frequenc f0 approxim polyphon music ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this large-scale study, consisting of 24.5 mil...</td>\n",
              "      <td>larg scale studi consist 24 5 million hand hyg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>we present a stability analysis of the plane c...</td>\n",
              "      <td>present stabil analysi plane couett flow stabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>we construct finite time blow-up solutions to ...</td>\n",
              "      <td>construct finit time blow solut 2 dimension ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>planetary nebulae (pne) constitute an importan...</td>\n",
              "      <td>planetari nebula pne constitut import tool stu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            ABSTRACT                              preprocessed_abstract\n",
              "0  fundamental frequency (f0) approximation from ...  fundament frequenc f0 approxim polyphon music ...\n",
              "1  this large-scale study, consisting of 24.5 mil...  larg scale studi consist 24 5 million hand hyg...\n",
              "2  we present a stability analysis of the plane c...  present stabil analysi plane couett flow stabl...\n",
              "3  we construct finite time blow-up solutions to ...  construct finit time blow solut 2 dimension ha...\n",
              "4  planetary nebulae (pne) constitute an importan...  planetari nebula pne constitut import tool stu..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC5b4-tJRedm"
      },
      "source": [
        "# combine all the text data to build vocabulary \n",
        "\n",
        "combined_vocab = list(train_data['preprocessed_abstract']) + list(test_data['preprocessed_abstract'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45vzpK9D5SSR",
        "outputId": "375306c8-95d6-47f4-dea3-b885a3f1b7de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# splitting train data -> train data + validation data\n",
        "\n",
        "X = train_data[['Computer Science', 'Mathematics', 'Physics', 'Statistics', 'preprocessed_abstract']]\n",
        "y = train_data[target_cols]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size = 0.25, random_state = 21)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_cv.shape, y_cv.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10503, 5) (10503, 25)\n",
            "(3501, 5) (3501, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhtZvbku_7WA",
        "outputId": "9116b69f-a233-4272-c3b6-3e5df10ee0c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TF-IDF Vectorization \n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.5, sublinear_tf = True, \n",
        "                             ngram_range = (1, 1))\n",
        "vectorizer.fit(combined_vocab)\n",
        "\n",
        "X_train_tfidf = vectorizer.transform(X_train['preprocessed_abstract'])\n",
        "X_cv_tfidf = vectorizer.transform(X_cv['preprocessed_abstract'])\n",
        "\n",
        "print(X_train_tfidf.shape, y_train.shape)\n",
        "print(X_cv_tfidf.shape, y_cv.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10503, 9136) (10503, 25)\n",
            "(3501, 9136) (3501, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2C-tcFBK03c",
        "outputId": "229133d4-1be4-4d3e-e372-1f2d280f6419",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# stacking the data for modeling\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "X_train_data_tfidf = hstack((X_train[topic_cols], X_train_tfidf))\n",
        "X_cv_data_tfidf = hstack((X_cv[topic_cols], X_cv_tfidf))\n",
        "\n",
        "print(X_train_data_tfidf.shape, y_train.shape)\n",
        "print(X_cv_data_tfidf.shape, y_cv.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10503, 9140) (10503, 25)\n",
            "(3501, 9140) (3501, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKi1SyoMxenJ",
        "outputId": "f9ae05c2-8d3c-48eb-e3f6-8bf0037cb117",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# hyperparameter tuning 'C' \n",
        "# One vs rest classifier with logistic regression \n",
        "%%time\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "C_range = [0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "for i in C_range:\n",
        "    clf = OneVsRestClassifier(LogisticRegression(C = i, solver = 'sag', n_jobs = -1))\n",
        "    clf.fit(X_train_data_tfidf, y_train)\n",
        "    y_pred_train = clf.predict(X_train_data_tfidf)\n",
        "    y_pred_cv = clf.predict(X_cv_data_tfidf)\n",
        "    f1_score_train = f1_score(y_train, y_pred_train, average = 'micro')\n",
        "    f1_score_cv = f1_score(y_cv, y_pred_cv, average = 'micro')\n",
        "\n",
        "    print(\"C:\", i, \"Train Score:\",f1_score_train, \"CV Score:\", f1_score_cv)\n",
        "    print(\"- \"*50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C: 0.01 Train Score: 0.2433689807262401 CV Score: 0.24125495852866932\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "C: 0.1 Train Score: 0.3222874237717179 CV Score: 0.316512186749056\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "C: 1 Train Score: 0.7698558981233244 CV Score: 0.6738360655737705\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "C: 10 Train Score: 0.9719875821240344 CV Score: 0.7467105263157896\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "C: 100 Train Score: 0.9999644469726597 CV Score: 0.7415220596574799\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "CPU times: user 6.43 s, sys: 622 ms, total: 7.05 s\n",
            "Wall time: 1min 37s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B41dLKmr2krA",
        "outputId": "2a7d02f6-2cfc-4682-c32b-779ef70250c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# hyperparameter tuning 'C' \n",
        "# One vs rest classifier with logistic regression \n",
        "\n",
        "C_range = [10, 20, 40, 70, 100]\n",
        "\n",
        "for i in C_range:\n",
        "    clf = OneVsRestClassifier(LogisticRegression(C = i, solver = 'sag', n_jobs = -1))\n",
        "    clf.fit(X_train_data_tfidf, y_train)\n",
        "    y_pred_train = clf.predict(X_train_data_tfidf)\n",
        "    y_pred_cv = clf.predict(X_cv_data_tfidf)\n",
        "    f1_score_train = f1_score(y_train, y_pred_train, average = 'micro')\n",
        "    f1_score_cv = f1_score(y_cv, y_pred_cv, average = 'micro')\n",
        "\n",
        "    print(\"C:\", i, \"Train Score:\",f1_score_train, \"CV Score:\", f1_score_cv)\n",
        "    print(\"- \"*50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C: 10 Train Score: 0.9719875821240344 CV Score: 0.7467105263157896\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "C: 20 Train Score: 0.9931206558545713 CV Score: 0.7493917274939172\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "C: 40 Train Score: 0.9986131360904662 CV Score: 0.7473081328751433\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "C: 70 Train Score: 0.999751093411087 CV Score: 0.7431557423605588\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "C: 100 Train Score: 0.9999644469726597 CV Score: 0.7417744497390515\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox18pI3WpgYD"
      },
      "source": [
        "Thus, C = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3oYjLX7M3vU"
      },
      "source": [
        "### Deciding the right threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clQrPqIJXdqD",
        "outputId": "45b541e2-d984-4047-b912-1d37fa4fd936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_cv_data_tfidf.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3501, 9140)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDxoaR24M06U"
      },
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "clf = OneVsRestClassifier(LogisticRegression(C = 20, solver = 'sag', n_jobs = -1)) \n",
        "clf.fit(X_train_data_tfidf, y_train)\n",
        "y_pred_train_proba = clf.predict_proba(X_train_data_tfidf)\n",
        "y_pred_cv_proba = clf.predict_proba(X_cv_data_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pakZ1lhM06g",
        "outputId": "2394830e-a06a-40ed-8a76-a30ca9236f4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "y_pred_cv_proba"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.79396592e-07, 2.20881182e-05, 1.71117371e-01, ...,\n",
              "        7.07265546e-05, 4.88317627e-05, 1.63590330e-01],\n",
              "       [1.45394901e-07, 4.96759791e-06, 7.05507139e-02, ...,\n",
              "        2.11107619e-05, 1.22927041e-05, 6.81844981e-03],\n",
              "       [3.60735599e-07, 1.47764047e-05, 1.73421181e-01, ...,\n",
              "        1.07032350e-05, 1.78404319e-05, 4.65502244e-03],\n",
              "       ...,\n",
              "       [1.95939520e-05, 9.95825228e-05, 4.81000587e-06, ...,\n",
              "        7.04150335e-01, 9.55637925e-01, 1.35204964e-06],\n",
              "       [9.91113560e-01, 9.76548738e-06, 5.47368033e-07, ...,\n",
              "        2.07254207e-03, 4.91099614e-03, 4.04460579e-05],\n",
              "       [5.42213984e-09, 1.43730395e-03, 1.52831734e-01, ...,\n",
              "        4.49009179e-06, 6.82246727e-06, 1.95454641e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKg3QDXZM06q"
      },
      "source": [
        "def get_best_thresholds(true, pred):\n",
        "    thresholds = [i/100 for i in range(100)]\n",
        "    best_thresholds = []\n",
        "    for idx in range(25):\n",
        "        f1_scores = [f1_score(true[:, idx], (pred[:, idx] > thresh) * 1) for thresh in thresholds]\n",
        "        best_thresh = thresholds[np.argmax(f1_scores)]\n",
        "        best_thresholds.append(best_thresh)\n",
        "    return best_thresholds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kgmPNYCM06z",
        "outputId": "5eb28d8c-960b-4e16-ab58-1c63f268c7e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "best_thresholds = get_best_thresholds(y_cv.values, y_pred_cv_proba)\n",
        "best_thresholds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.45, 0.28, 0.19, 0.46, 0.24, 0.24, 0.24, 0.28, 0.22, 0.2, 0.22, 0.24, 0.24, 0.41, 0.32, 0.15, 0.21, 0.33, 0.33, 0.29, 0.16, 0.66, 0.33, 0.36, 0.4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os_hnlYnjKnx",
        "outputId": "d6ec273b-1b21-4870-c034-f877393f770b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred_cv = np.empty_like(y_pred_cv_proba)\n",
        "for i, thresh in enumerate(best_thresholds):\n",
        "    y_pred_cv[:, i] = (y_pred_cv_proba[:, i] > thresh) * 1\n",
        "  \n",
        "print(f1_score(y_cv, y_pred_cv, average = 'micro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7765116517811312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpbOkGIKpoZz"
      },
      "source": [
        "Using above found optimal value of regularization parameter (C = 20) and best thresholds "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61OG7BWJkiPl",
        "outputId": "580edf69-345c-4049-ebe3-2a261e10eaee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_tr = train_data[['Computer Science', 'Mathematics', 'Physics', 'Statistics', 'preprocessed_abstract']]\n",
        "y_tr = train_data[target_cols]\n",
        "\n",
        "X_te = test_data[['Computer Science', 'Mathematics', 'Physics', 'Statistics', 'preprocessed_abstract']]\n",
        "print(X_tr.shape, y_tr.shape)\n",
        "print(X_te.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14004, 5) (14004, 25)\n",
            "(6002, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhad0BLNTvo4",
        "outputId": "375e317c-912c-4e68-f1dd-39d4102bc8de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_tr_tfidf = vectorizer.transform(X_tr['preprocessed_abstract'])\n",
        "X_te_tfidf = vectorizer.transform(X_te['preprocessed_abstract'])\n",
        "\n",
        "print(X_tr_tfidf.shape, y_tr.shape)\n",
        "print(X_te_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14004, 9136) (14004, 25)\n",
            "(6002, 9136)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kby1Ud6CkiQT",
        "outputId": "0e232001-3c5e-4dbd-f6bb-4679607e5d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# TF-IDF Vectorization \n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.5, sublinear_tf = True, \n",
        "                             ngram_range = (1, 1), max_features = 10000)\n",
        "vectorizer.fit(combined_vocab)\n",
        "\n",
        "X_tr_tfidf = vectorizer.transform(X_tr['preprocessed_abstract'])\n",
        "X_te_tfidf = vectorizer.transform(X_te['preprocessed_abstract'])\n",
        "\n",
        "print(X_tr_tfidf.shape, y_tr.shape)\n",
        "print(X_te_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14004, 9136) (14004, 25)\n",
            "(6002, 9136)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEnerTeQpeyO",
        "outputId": "2c77d41a-f307-4d39-8032-9b0d13900e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# stacking the data for modeling\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "X_tr_data_tfidf = hstack((X_tr[topic_cols], X_tr_tfidf))\n",
        "X_te_data_tfidf = hstack((X_te[topic_cols], X_te_tfidf))\n",
        "\n",
        "print(X_tr_data_tfidf.shape, y_tr.shape)\n",
        "print(X_te_data_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14004, 9140) (14004, 25)\n",
            "(6002, 9140)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRCkN7mmIk1n"
      },
      "source": [
        "### Logistic Regression with TFIDF L2 C = 20 (with thresholding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KZ_HwdNIk1r"
      },
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "clf = OneVsRestClassifier(LogisticRegression(C = 20, n_jobs = -1)) # default penalty is L2\n",
        "clf.fit(X_tr_data_tfidf, y_tr)\n",
        "y_pred_tr_proba = clf.predict_proba(X_tr_data_tfidf)\n",
        "y_pred_te_proba = clf.predict_proba(X_te_data_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeayDrYSLMIt",
        "outputId": "4384d002-7721-47d2-bd1e-f11e8ab0a9e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "y_pred_te_proba"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.05520039e-07, 9.59804726e-04, 1.08296313e-08, ...,\n",
              "        1.92653374e-05, 2.48607642e-04, 7.71172547e-09],\n",
              "       [2.63356797e-08, 8.48443851e-01, 1.17337157e-01, ...,\n",
              "        8.17892637e-06, 3.69808263e-05, 5.52674059e-03],\n",
              "       [5.38299182e-05, 1.01343264e-05, 1.41326654e-10, ...,\n",
              "        1.11015106e-03, 2.15385563e-03, 6.10980310e-06],\n",
              "       ...,\n",
              "       [5.69224406e-08, 7.05548880e-07, 9.24906477e-01, ...,\n",
              "        1.56863163e-06, 5.75309697e-06, 2.48092902e-04],\n",
              "       [1.35092796e-08, 9.53375721e-02, 2.60804759e-09, ...,\n",
              "        9.86714738e-05, 9.02050594e-05, 2.13402569e-08],\n",
              "       [1.14110881e-04, 1.40002020e-05, 3.61553677e-09, ...,\n",
              "        6.48231699e-01, 3.85914853e-02, 3.68965211e-06]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njoNllSQLxXi"
      },
      "source": [
        "y_pred_te = np.empty_like(y_pred_te_proba)\n",
        "for i, thresh in enumerate(best_thresholds):\n",
        "    y_pred_te[:, i] = (y_pred_te_proba[:, i] > thresh) * 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGaDIoU0Ik13",
        "outputId": "dd0a3002-6a35-4441-8542-6f8883a54ae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "pd.DataFrame(y_pred_te).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0    1    2    3    4    5    6   ...   18   19   20   21   22   23   24\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "1  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "4  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uycj0mLInBAF"
      },
      "source": [
        "# make submission \n",
        "\n",
        "ss = pd.read_csv(my_path + 'SampleSubmission.csv')\n",
        "ss[target_cols] = y_pred_te\n",
        "ss.to_csv(my_path + 'combined1000_LR_tfidf_L2_C20_thresh.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc5DSW1mrkRE"
      },
      "source": [
        "micro-F1 Score upon submission: 0.7685393258 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G3stX2TXg_Y"
      },
      "source": [
        "### Logistic Regression with TFIDF L2 C = 10 (with Classifier Chains and  thresholding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRdXIBYiXg_d"
      },
      "source": [
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "clf = ClassifierChain(LogisticRegression(C = 10, n_jobs = -1), random_state = 21) # default penalty is L\n",
        "clf.fit(X_tr_data_tfidf, y_tr)\n",
        "y_pred_tr_proba = clf.predict_proba(X_tr_data_tfidf)\n",
        "y_pred_te_proba = clf.predict_proba(X_te_data_tfidf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB3pPFBNXg_q",
        "outputId": "9e837578-a4b4-42d1-8043-d9ae2b62cdb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "y_pred_te_proba"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.31984152e-07, 5.06627026e-04, 4.10243757e-05, ...,\n",
              "        1.58994516e-04, 6.11620816e-04, 1.19570269e-07],\n",
              "       [8.16695539e-08, 7.26189463e-01, 3.09424803e-02, ...,\n",
              "        9.13965273e-06, 5.99373480e-05, 2.32652375e-01],\n",
              "       [1.41328160e-04, 2.46281488e-05, 5.55569691e-07, ...,\n",
              "        1.95032517e-03, 2.04194393e-03, 1.38320284e-05],\n",
              "       ...,\n",
              "       [3.12909195e-07, 2.52342291e-06, 9.08481907e-01, ...,\n",
              "        6.31078729e-05, 3.82337011e-05, 4.25089660e-04],\n",
              "       [1.22096572e-07, 1.46161640e-01, 7.60009066e-05, ...,\n",
              "        1.39827582e-04, 1.42394371e-04, 1.62597344e-07],\n",
              "       [2.93676575e-04, 3.60657695e-05, 4.67210371e-05, ...,\n",
              "        8.23344618e-01, 8.21029626e-02, 1.21703368e-05]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZlompp8Xg_1"
      },
      "source": [
        "y_pred_te = np.empty_like(y_pred_te_proba)\n",
        "for i, thresh in enumerate(best_thresholds):\n",
        "    y_pred_te[:, i] = (y_pred_te_proba[:, i] > thresh) * 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dfJRTclXhAB",
        "outputId": "e82708a6-484e-430e-938c-5882307cacd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "pd.DataFrame(y_pred_te).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0    1    2    3    4    5    6   ...   18   19   20   21   22   23   24\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "1  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "4  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N25qf-L8XhAL"
      },
      "source": [
        "# make submission\n",
        "ss = pd.read_csv(my_path + 'SampleSubmission.csv')\n",
        "ss[target_cols] = y_pred_te\n",
        "ss.to_csv(my_path + 'chain_LR_tfidf_L2_C10_thresh.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CmWCJ0Lrecf"
      },
      "source": [
        "micro-F1 Score upon submission: 0.749537892791128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLcXWmQGqTKB"
      },
      "source": [
        "Among Classifier Chains and OnevsRestClassifier, OnevsRestClassifier gave better peformance on the test set. \n"
      ]
    }
  ]
}